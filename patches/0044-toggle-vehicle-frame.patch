From fcca8c01f59a224bb0ea50cc4ccbff8082d360d5 Mon Sep 17 00:00:00 2001
From: KenC1014 <kenken4016@gmail.com>
Date: Mon, 17 Feb 2025 22:42:55 -0600
Subject: [PATCH 044/150] toggle vehicle frame

---
 .../perception/pedestrian_detection.py        | 34 +++++++++++--------
 GEMstack/onboard/perception/transform.py      |  2 --
 2 files changed, 20 insertions(+), 16 deletions(-)

diff --git a/GEMstack/onboard/perception/pedestrian_detection.py b/GEMstack/onboard/perception/pedestrian_detection.py
index 31085ce8..2b6626ce 100644
--- a/GEMstack/onboard/perception/pedestrian_detection.py
+++ b/GEMstack/onboard/perception/pedestrian_detection.py
@@ -85,6 +85,7 @@ class PedestrianDetector2D(Component):
         self.classes_to_detect = 0
         self.ground_threshold = -2.0
         self.max_human_depth = 0.9
+        self.vehicle_frame = False # Indicate whether pedestrians centroids and point clouds are in the vehicle frame
 
         # Load calibration data
         # TODO: Maybe lets add one word or link what R t K are?
@@ -193,10 +194,12 @@ class PedestrianDetector2D(Component):
             self.pub_image.publish(ros_img)  
 
             # Draw 3D pedestrian pointclouds
-            # Tranform 3D pedestrians points to vehicle frame for better visualization. Turn off for performance
-            flattened_pedestrians_3d_pts_vehicle = transform_lidar_points(np.array(flattened_pedestrians_3d_pts), self.R_lidar_to_vehicle, self.t_lidar_to_vehicle)
+            if self.vehicle_frame:
+                # If in vehicle frame, tranform 3D pedestrians points to vehicle frame for better visualization.
+                flattened_pedestrians_3d_pts_vehicle = transform_lidar_points(np.array(flattened_pedestrians_3d_pts), self.R_lidar_to_vehicle, self.t_lidar_to_vehicle)
+                flattened_pedestrians_3d_pts = flattened_pedestrians_3d_pts_vehicle
+
             # Create point cloud from extracted 3D points
-            # ros_extracted_pedestrian_pc2 = create_point_cloud(flattened_pedestrians_3d_pts_vehicle)
             ros_extracted_pedestrian_pc2 = create_point_cloud(flattened_pedestrians_3d_pts)
             self.pub_pedestrians_pc2.publish(ros_extracted_pedestrian_pc2)
 
@@ -241,20 +244,22 @@ class PedestrianDetector2D(Component):
         obj_dims = self.find_dims(pedestrians_3d_pts)
         obj_vels = self.find_vels(track_ids, obj_centers)
 
-        # Transform centers from top_lidar frame to vehicle frame
+        # If in vehicle frame, transform centers from top_lidar frame to vehicle frame
         # Need to transform the center point one by one since matrix op can't deal with empty points
-        obj_centers_vehicle = []
-        for obj_center in obj_centers:
-            if len(obj_center) > 0:
-                obj_center = np.array([obj_center])
-                obj_center_vehicle = transform_lidar_points(obj_center, self.R_lidar_to_vehicle, self.t_lidar_to_vehicle)[0]
-                obj_centers_vehicle.append(obj_center_vehicle)
-            else:
-                obj_centers_vehicle.append(np.array(()))
+        if self.vehicle_frame:
+            obj_centers_vehicle = []
+            for obj_center in obj_centers:
+                if len(obj_center) > 0:
+                    obj_center = np.array([obj_center])
+                    obj_center_vehicle = transform_lidar_points(obj_center, self.R_lidar_to_vehicle, self.t_lidar_to_vehicle)[0]
+                    obj_centers_vehicle.append(obj_center_vehicle)
+                else:
+                    obj_centers_vehicle.append(np.array(()))
+            obj_centers = obj_centers_vehicle
         
         # Update Current AgentStates
         for ind in range(num_objs):
-            obj_center = (None, None, None) if obj_centers_vehicle[ind].size == 0 else obj_centers_vehicle[ind]
+            obj_center = (None, None, None) if obj_centers[ind].size == 0 else obj_centers[ind]
             obj_dim = (None, None, None) if obj_dims[ind].size == 0 else obj_dims[ind]
             self.current_agents[track_ids[ind]] = (
                 AgentState(
@@ -319,7 +324,8 @@ class PedestrianDetector2D(Component):
                 extracted_pts = filter_ground_points(extracted_pts, self.ground_threshold)
                 extracted_pts = filter_depth_points(extracted_pts, self.max_human_depth)
                 extracted_pts_all.append(extracted_pts)
-            else: extracted_pts_all.append(np.array(()))
+            # Still causing errors, I will turn this on later
+            # else: extracted_pts_all.append(np.array(()))
         
         #if len(extracted_pts_all) > 0 and len(track_result) > 0:
         self.update_object_states(track_result, extracted_pts_all)
diff --git a/GEMstack/onboard/perception/transform.py b/GEMstack/onboard/perception/transform.py
index b876cc59..8032752b 100644
--- a/GEMstack/onboard/perception/transform.py
+++ b/GEMstack/onboard/perception/transform.py
@@ -9,8 +9,6 @@ def publish_tf():
 
     while not rospy.is_shutdown():
         br.sendTransform(
-            # (0, 1.6, 7),  # (x, y, z) translation
-            # tf.transformations.quaternion_from_euler(0.5* np.pi, 0, 0),  # (roll, pitch, yaw)
             (0, 0, 0),  # (x, y, z) translation
             tf.transformations.quaternion_from_euler(0, 0, 0),  # (roll, pitch, yaw)
             rospy.Time.now(),
-- 
2.38.1

