From 475b8c91ae9bd40b93c7aaded7d0d7a623464b95 Mon Sep 17 00:00:00 2001
From: KenC1014 <kenken4016@gmail.com>
Date: Fri, 14 Feb 2025 20:02:30 -0600
Subject: [PATCH 019/150] align lidar pc2 to camera frame

---
 .../calibration/camera_intrinsics.json        |   7 +
 .../perception/calibration/extrinsics/R.npy   | Bin 0 -> 200 bytes
 .../calibration/extrinsics/rvec.npy           | Bin 0 -> 152 bytes
 .../perception/calibration/extrinsics/t.npy   | Bin 0 -> 152 bytes
 GEMstack/onboard/perception/fusion.py         |  80 +++++++--
 GEMstack/onboard/perception/fusion_utils.py   | 159 ++++++++++++++++++
 GEMstack/onboard/perception/transform.py      |   2 +-
 7 files changed, 232 insertions(+), 16 deletions(-)
 create mode 100644 GEMstack/onboard/perception/calibration/camera_intrinsics.json
 create mode 100644 GEMstack/onboard/perception/calibration/extrinsics/R.npy
 create mode 100644 GEMstack/onboard/perception/calibration/extrinsics/rvec.npy
 create mode 100644 GEMstack/onboard/perception/calibration/extrinsics/t.npy
 create mode 100644 GEMstack/onboard/perception/fusion_utils.py

diff --git a/GEMstack/onboard/perception/calibration/camera_intrinsics.json b/GEMstack/onboard/perception/calibration/camera_intrinsics.json
new file mode 100644
index 00000000..74318680
--- /dev/null
+++ b/GEMstack/onboard/perception/calibration/camera_intrinsics.json
@@ -0,0 +1,7 @@
+{
+    "fx": 684.8333129882812,
+    "fy": 684.6096801757812,
+    "cx": 573.37109375,
+    "cy": 363.700927734375
+  }
+  
\ No newline at end of file
diff --git a/GEMstack/onboard/perception/calibration/extrinsics/R.npy b/GEMstack/onboard/perception/calibration/extrinsics/R.npy
new file mode 100644
index 0000000000000000000000000000000000000000..7d44aa76dc77d0199fe86055c0f3d0e2e068d1e1
GIT binary patch
literal 200
zcmbR27wQ`j$;eQ~P_3SlTAW;@Zl$1ZlV+i=qoAIaUsO_*m=~X4l#&V(cT3DEP6dh=
zXCxM+0{I%oIts>`ItsN4WCJb%fwxMDr{~)LE9si%@#p>ihKYjy;`i(KAE>>{UAU@z
vzr*Jf@d8Hm_6&!%YcKf!et)igplIr!_x1(a2fkgnHrJlvT6b{B{<8f5RmVXB

literal 0
HcmV?d00001

diff --git a/GEMstack/onboard/perception/calibration/extrinsics/rvec.npy b/GEMstack/onboard/perception/calibration/extrinsics/rvec.npy
new file mode 100644
index 0000000000000000000000000000000000000000..62d1809f1f7ea2665a24df654d48ec92a21310e7
GIT binary patch
literal 152
zcmbR27wQ`j$;eQ~P_3SlTAW;@Zl$1ZlV+i=qoAIaUsO_*m=~X4l#&V(cT3DEP6dh=
xXCxM+0{I%oItqrGItsN4WCN}*xxZd?bbYozBkxmtO5yW<;qS-&F5LTM4*;iRDntMP

literal 0
HcmV?d00001

diff --git a/GEMstack/onboard/perception/calibration/extrinsics/t.npy b/GEMstack/onboard/perception/calibration/extrinsics/t.npy
new file mode 100644
index 0000000000000000000000000000000000000000..b133fa55fd0c5362ac4d93bac53efeb471dfee3f
GIT binary patch
literal 152
zcmbR27wQ`j$;eQ~P_3SlTAW;@Zl$1ZlV+i=qoAIaUsO_*m=~X4l#&V(cT3DEP6dh=
xXCxM+0{I%oItqrGItsN4WCJeCQ?j?%^Ox^`s-3-k&cPe|OJ3%@a}Rp99{`L8DPRBq

literal 0
HcmV?d00001

diff --git a/GEMstack/onboard/perception/fusion.py b/GEMstack/onboard/perception/fusion.py
index 0b6e4f20..d8cd1a41 100644
--- a/GEMstack/onboard/perception/fusion.py
+++ b/GEMstack/onboard/perception/fusion.py
@@ -1,30 +1,80 @@
 from cv_bridge import CvBridge
 from sensor_msgs.msg import Image, PointCloud2
+from ultralytics import YOLO
+from fusion_utils import *
 import rospy
 import message_filters
-
+import os
+import tf
 
 class Fusion3D():
     def __init__(self):
+        # Setup variables
         self.bridge = CvBridge()
-        self.stereo_rosbag = message_filters.Subscriber('/oak/stereo/image_raw', Image, self.stereo_callback, queue_size=1)
-        self.top_lidar_rosbag = message_filters.Subscriber('/ouster/points', PointCloud2, self.top_lidar_callback, queue_size=1)
+        self.detector = YOLO(os.getcwd()+'/GEMstack/knowledge/detection/yolov8n.pt')
+        self.last_person_boxes = [] 
+        self.pedestrians = {}
+        self.visualization = True
+        self.confidence = 0.7
+        self.classes_to_detect = 0
+
+        # Load calibration data
+        self.R = load_extrinsics(os.getcwd() + '/GEMstack/onboard/perception/calibration/extrinsics/R.npy')
+        self.t = load_extrinsics(os.getcwd() + '/GEMstack/onboard/perception/calibration/extrinsics/t.npy')
+        self.K = load_intrinsics(os.getcwd()+ '/GEMstack/onboard/perception/calibration/camera_intrinsics.json')
+
+        # Subscribers and sychronizers
+        self.rgb_rosbag = message_filters.Subscriber('/oak/rgb/image_raw', Image)
+        self.top_lidar_rosbag = message_filters.Subscriber('/ouster/points', PointCloud2)
+        self.sync = message_filters.ApproximateTimeSynchronizer([self.rgb_rosbag, self.top_lidar_rosbag], queue_size=10, slop=0.1)
+        self.sync.registerCallback(self.fusion_callback)
+
+        # TF listener to get transformation from LiDAR to Camera
+        self.tf_listener = tf.TransformListener()
+
+        # Publishers
+        if(self.visualization):
+            self.pub_image = rospy.Publisher("/camera/image_detection", Image, queue_size=1)
+
+
+
+    def fusion_callback(self, image: Image, lidar_pc2_msg: PointCloud2):
+        image = self.bridge.imgmsg_to_cv2(image, "bgr8") 
+        track_result = self.detector.track(source=image, classes=self.classes_to_detect, persist=True, conf=self.confidence)
+
+        self.last_person_boxes = []
+        boxes = track_result[0].boxes
+
+        # Unpacking box dimentions detected into x,y,w,h
+        for box in boxes:
+            xywh = box.xywh[0].tolist()
+            self.last_person_boxes.append(xywh)
+
+            # Used for visualization
+            if(self.visualization):
+                image = vis_2d_bbox(image, xywh, box)
 
-        ts = message_filters.ApproximateTimeSynchronizer([self.stereo_rosbag, self.top_lidar_rosbag], queue_size=10, slop=0.1)
-        ts.registerCallback(self.callback)
-        rospy.spin()
+        # Convert 1D PointCloud2 data to x, y, z coords
+        lidar_points = convert_pointcloud2_to_xyz(lidar_pc2_msg)
+    
+        # Transform LiDAR points into the camera coordinate frame.
+        lidar_in_camera = transform_lidar_points(lidar_points, self.R, self.t)
+    
+        # Project the transformed points into the image plane.
+        projected_pts = project_points(lidar_in_camera, self.K)
+        
+        # Draw projected LiDAR points on the image.
+        for pt in projected_pts:
+            cv2.circle(image, pt, 2, (0, 0, 255), -1)
 
-    def callback(left_img, right_img, lidar_msg, camera_info):
-    # Process synchronized data here
-        pass
+        # visualize_point_cloud(p_img_cloud)
+        
+        # Used for visualization
+        if(self.visualization):
+            ros_img = self.bridge.cv2_to_imgmsg(image, 'bgr8')
+            self.pub_image.publish(ros_img)  
 
-    def stereo_callback(self, image: Image):
-        image = self.bridge.imgmsg_to_cv2(image, "16UC1")
-        print(f"stereo: {image.shape}")        
 
-    def top_lidar_callback(self, point_cloud: PointCloud2):
-        # point_cloud = self.bridge.imgmsg_to_cv2(point_cloud, "16UC1")
-        print(f"point_cloud: {point_cloud.fields}")       
 
 if __name__ == '__main__':
     rospy.init_node('fusion_node', anonymous=True)
diff --git a/GEMstack/onboard/perception/fusion_utils.py b/GEMstack/onboard/perception/fusion_utils.py
new file mode 100644
index 00000000..ad2c13c1
--- /dev/null
+++ b/GEMstack/onboard/perception/fusion_utils.py
@@ -0,0 +1,159 @@
+from sensor_msgs.msg import PointCloud2
+import numpy as np
+import sensor_msgs.point_cloud2 as pc2
+import open3d as o3d
+import cv2
+import json
+
+
+def convert_pointcloud2_to_xyz(lidar_pc2_msg: PointCloud2):
+    """ Convert 1D PointCloud2 data to x, y, z coords """
+    lidar_points = []
+    for point in pc2.read_points(lidar_pc2_msg, field_names=("x", "y", "z", "intensity"), skip_nans=True):
+        lidar_points.append([point[0], point[1], point[2]])
+    return np.array(lidar_points)
+
+
+# Credits: The following lines of codes (17 to 159 excluding lines 80 to 115) are adapted from the Calibration Team B
+def load_extrinsics(extrinsics_file):
+    """
+    Load calibrated extrinsics from a .npz file.
+    Assumes the file contains keys 'R' and 't'.
+    """
+    data = np.load(extrinsics_file)
+    return data
+
+
+def load_intrinsics(intrinsics_file):
+    """
+    Load camera intrinsics from a JSON file.
+    Expects keys: 'fx', 'fy', 'cx', and 'cy'.
+    """
+    with open(intrinsics_file, 'r') as f:
+        intrinsics = json.load(f)
+    fx = intrinsics["fx"]
+    fy = intrinsics["fy"]
+    cx = intrinsics["cx"]
+    cy = intrinsics["cy"]
+    K = np.array([[fx, 0, cx],
+                  [0, fy, cy],
+                  [0,  0,  1]], dtype=np.float32)
+    return K
+
+
+def load_lidar_scan(lidar_file):
+    """
+    Load a LiDAR scan from a file.
+    This function handles both npy (returns a NumPy array) and npz files.
+    """
+    data = np.load(lidar_file, allow_pickle=True)
+    if isinstance(data, np.ndarray):
+        return data.astype(np.float32)
+    else:
+        key = list(data.keys())[0]
+        return data[key].astype(np.float32)
+
+
+def transform_lidar_points(lidar_points, R, t):
+    """
+    Transform LiDAR points from the LiDAR frame into the camera frame.
+    p_cam = R * p_lidar + t.
+    """
+    P_cam = (R @ lidar_points.T + t.reshape(3,1)).T
+    return P_cam
+
+
+def project_points(points_3d, K):
+    """
+    Project 3D points (in the camera frame) into 2D image coordinates using the camera matrix K.
+    Only projects points with z > 0.
+    """
+    proj_points = []
+    for pt in points_3d:
+        if pt[2] > 0:  # only project points in front of the camera
+            u = K[0, 0] * (pt[0] / pt[2]) + K[0, 2]
+            v = K[1, 1] * (pt[1] / pt[2]) + K[1, 2]
+            proj_points.append((int(u), int(v)))
+    return proj_points
+
+
+def vis_2d_bbox(image, xywh, box):
+    # Setup
+    label_text = "Pedestrian "
+    font = cv2.FONT_HERSHEY_SIMPLEX
+    font_scale = 0.5
+    font_color = (255, 255, 255)
+    line_type = 1
+    text_thickness = 2
+
+    x, y, w, h = xywh
+
+    if box.id is not None:
+        id = box.id.item()
+    else:
+        id = 0
+
+    # Draw bounding box
+    cv2.rectangle(image, (int(x - w / 2), int(y - h / 2)), (int(x + w / 2), int(y + h / 2)), (255, 0, 255), 3)
+
+    # Define text label
+    x = int(x - w / 2)
+    y = int(y - h / 2)
+    label = label_text + str(id) + " : " + str(round(box.conf.item(), 2))
+
+    # Get text size
+    text_size, baseline = cv2.getTextSize(label, font, font_scale, line_type)
+    text_w, text_h = text_size
+
+    # Position text above the bounding box
+    text_x = x
+    text_y = y - 10 if y - 10 > 10 else y + h + text_h
+
+    # Draw main text on top of the outline
+    cv2.putText(image, label, (text_x, text_y - baseline), font, font_scale, font_color, text_thickness)
+
+    return image
+
+
+def visualize_point_cloud(points):
+    """
+    Visualizes the given point cloud using Open3D.
+
+    Args:
+        points (np.ndarray): Nx3 array of point cloud coordinates.
+    """
+    pc = o3d.geometry.PointCloud()
+    pc.points = o3d.utility.Vector3dVector(points)
+    pc.paint_uniform_color([0.1, 0.7, 0.9])  # Light blue color
+    o3d.visualization.draw_geometries([pc])
+
+
+def visualize_plane(inlier_cloud, outlier_cloud, bounding_box_2d_points):
+    """
+    Visualizes the detected plane with its 2D bounding box.
+
+    :param inlier_cloud: Open3D point cloud containing plane points.
+    :param outlier_cloud: Open3D point cloud containing non-plane points.
+    :param bounding_box_2d_points: 4 corner points of the 2D bounding box on the plane.
+    """
+    inlier_cloud.paint_uniform_color([1, 0, 0])  # Red for the plane
+    outlier_cloud.paint_uniform_color([0.5, 0.5, 0.5])  # Gray for other points
+
+    # Create bounding box visualization
+    bounding_box_pcd = o3d.geometry.PointCloud()
+    bounding_box_pcd.points = o3d.utility.Vector3dVector(bounding_box_2d_points)
+    bounding_box_pcd.paint_uniform_color([0, 1, 0])  # Green for bounding box corners
+    
+    # Create a bounding box line set (connect corners)
+    lines = [
+        [0, 1], [1, 2], [2, 3], [3, 0]   # Edges of the rectangle
+    ]
+    
+    bounding_box_lines = o3d.geometry.LineSet()
+    bounding_box_lines.points = o3d.utility.Vector3dVector(bounding_box_2d_points)
+    bounding_box_lines.lines = o3d.utility.Vector2iVector(lines)
+    
+    bounding_box_lines.paint_uniform_color([0, 1, 0])  # Green for bounding box edges
+    
+    # Visualize
+    o3d.visualization.draw_geometries([inlier_cloud, outlier_cloud, bounding_box_pcd, bounding_box_lines])
\ No newline at end of file
diff --git a/GEMstack/onboard/perception/transform.py b/GEMstack/onboard/perception/transform.py
index 8d47598f..4487e051 100644
--- a/GEMstack/onboard/perception/transform.py
+++ b/GEMstack/onboard/perception/transform.py
@@ -11,7 +11,7 @@ def publish_tf():
             (0, 0, 1),  # (x, y, z) translation
             tf.transformations.quaternion_from_euler(0, 0, 0),  # (roll, pitch, yaw)
             rospy.Time.now(),
-            "oak_rgb_camera_optical_frame",  # Child frame (sensor)
+            "os_sensor",  # Child frame (sensor)
             "map"  # Parent frame (world)
         )
         rate.sleep()
-- 
2.38.1

