From b62c6fc60e6e6a1b8d9ab782e14b097b0a06ce27 Mon Sep 17 00:00:00 2001
From: KenC1014 <kenken4016@gmail.com>
Date: Mon, 17 Feb 2025 15:15:55 -0600
Subject: [PATCH 039/150] integrate updated depth filter and 3D bboxes vis

---
 GEMstack/onboard/perception/fusion.py         | 270 ------------------
 .../perception/pedestrian_detection.py        |  88 ++++--
 .../perception/pedestrian_detection_utils.py  |  82 +++++-
 3 files changed, 135 insertions(+), 305 deletions(-)
 delete mode 100644 GEMstack/onboard/perception/fusion.py

diff --git a/GEMstack/onboard/perception/fusion.py b/GEMstack/onboard/perception/fusion.py
deleted file mode 100644
index b642a062..00000000
--- a/GEMstack/onboard/perception/fusion.py
+++ /dev/null
@@ -1,270 +0,0 @@
-"""
-Top ouster lidar + Oak front camera fusion, object detection
-"""
-"""
-Terminal 1:
----------------
-source /opt/ros/noetic/setup.bash && source /catkin_ws/devel/setup.bash
-roscore
-
-Terminal 2:
----------------
-source /opt/ros/noetic/setup.bash && source /catkin_ws/devel/setup.bash
-python3 GEMstack/onboard/perception/transform.py
-
-Terminal 3:
----------------
-source /opt/ros/noetic/setup.bash && source /catkin_ws/devel/setup.bash
-rosbag play -l ~/rosbags/vehicle.bag
-
-Terminal 4:
----------------
-source /opt/ros/noetic/setup.bash && source /catkin_ws/devel/setup.bash
-cd GEMStack
-python3 -m GEMstack.onboard.perception.fusion
-
-Terminal 5:
----------------
-source /opt/ros/noetic/setup.bash && source /catkin_ws/devel/setup.bash
-rviz
-"""
-
-# Python 
-import os
-from typing import List, Dict
-from collections import defaultdict
-# ROS, CV
-import rospy
-import message_filters
-import tf
-from cv_bridge import CvBridge
-from sensor_msgs.msg import Image, PointCloud2
-# GEMStack
-from ...state import AllState,VehicleState,ObjectPose,ObjectFrameEnum,AgentState,AgentEnum,AgentActivityEnum
-from ultralytics import YOLO
-from .pedestrian_detection_utils import *
-
-class Fusion3D():
-    # TODO: Pull params into a JSON/yaml
-    # TODO: Convert some lists into np.arrays, vectorize calculations
-    # TODO: Implement logging instead of print, cleanup comments
-    # TODO: Cleanup funcs + split into separate classes
-    # TODO: Decide if we want to name dets "peds" or "objs"/"agents"
-    #       Maybe peds for now and Agents in agent_detection.py?
-    def __init__(self):
-        # Publish debug/viz rostopics if true
-        self.debug = True
-        # Setup variables
-        self.bridge = CvBridge()
-        # TODO: Wrap detector into GEMDetector?
-        self.detector = YOLO(os.getcwd()+'/GEMstack/knowledge/detection/yolov8n.pt')
-        # track_id: AgentState
-        self.prev_agents = dict()         
-        self.current_agents = dict()
-        self.confidence = 0.7
-        self.classes_to_detect = 0
-        self.ground_threshold = 1.6
-        self.max_dist_percent = 0.7
-
-        # Load calibration data
-        self.R = load_extrinsics(os.getcwd() + '/GEMstack/onboard/perception/calibration/extrinsics/R.npy')
-        self.t = load_extrinsics(os.getcwd() + '/GEMstack/onboard/perception/calibration/extrinsics/t.npy')
-        self.K = load_intrinsics(os.getcwd() + '/GEMstack/onboard/perception/calibration/camera_intrinsics.json')
-
-        # Subscribers and sychronizers
-        self.rgb_rosbag = message_filters.Subscriber('/oak/rgb/image_raw', Image)
-        self.top_lidar_rosbag = message_filters.Subscriber('/ouster/points', PointCloud2)
-        self.sync = message_filters.ApproximateTimeSynchronizer([self.rgb_rosbag, self.top_lidar_rosbag], queue_size=10, slop=0.1)
-        self.sync.registerCallback(self.ouster_oak_callback)
-
-        # TF listener to get transformation from LiDAR to Camera
-        self.tf_listener = tf.TransformListener()
-
-        if self.debug: self.init_debug()
-    
-    def init_debug(self,):
-         # Debug Publishers
-        self.pub_pedestrians_pc2 = rospy.Publisher("/point_cloud/pedestrians", PointCloud2, queue_size=10)
-        self.pub_obj_centers_pc2 = rospy.Publisher("/point_cloud/obj_centers", PointCloud2, queue_size=10)
-        self.pub_image = rospy.Publisher("/camera/image_detection", Image, queue_size=1)
-
-    def update(self, vehicle : VehicleState) -> Dict[str,AgentState]:
-        return self.current_agents
-
-    # TODO: Improve Algo Knn, ransac, etc.
-    def find_centers(self, clusters: List[List[np.ndarray]]) -> List[np.ndarray]:
-        clusters = [np.array(clust) for clust in clusters]
-        centers = [np.array(()) if clust.size == 0 else np.mean(clust, axis=0) for clust in clusters]
-        return centers
-    
-    # Beware: AgentState(PhysicalObject) builds bbox from 
-    # dims [-l/2,l/2] x [-w/2,w/2] x [0,h], not
-    # [-l/2,l/2] x [-w/2,w/2] x [-h/2,h/2]
-    def find_dims(self, clusters: List[List[np.ndarray]]) -> List[np.ndarray]:
-        # Add some small constant to height to compensate for
-        # objects distance to ground we filtered from lidar, 
-        # other heuristics to imrpove stability for find_ funcs ?
-        clusters = [np.array(clust) for clust in clusters]
-        dims = [np.array(()) if clust.size == 0 else np.max(clust, axis= 0) - np.min(clust, axis= 0) for clust in clusters]
-        return dims
-
-    # TODO: Slower but cleaner to input self.current_agents dict
-    # TODO: Moving Average across last N iterations pos/vel? Less spurious vals
-    # TODO Akul: Fix velocity calculation to calculate in ObjectFrameEnum.START
-    #            work towards own tracking class instead of simple YOLO track?
-    # ret: Dict[track_id: vel[x, y, z]]
-    def find_vels(self, track_ids, obj_centers):
-        # Object not seen -> velocity = None
-        track_id_center_map = dict(zip(track_ids, obj_centers))
-        vels = defaultdict(lambda: np.array(())) # None is faster, np.array matches other find_ methods.
-
-        for prev_track_id, prev_agent in self.prev_agents.items():
-            if prev_track_id in track_ids:
-                # TODO: Add prev_agents to memory to avoid None velocity
-                # We should only be missing prev pose on first sight of track_id Agent.
-                # print("shape 1: ", track_id_center_map[prev_agent.track_id])
-                # print("shape 2: ", np.array([prev_agent.pose.x, prev_agent.pose.y, prev_agent.pose.z]))
-                # prev can be 3 separate Nones, current is just empty array... make this symmetrical
-                if prev_agent.pose.x and prev_agent.pose.y and prev_agent.pose.z and track_id_center_map[prev_agent.track_id].shape == 3:
-                    vels[prev_track_id] = track_id_center_map[prev_track_id] - np.array([prev_agent.pose.x, prev_agent.pose.y, prev_agent.pose.z])
-        return vels
-
-
-    # TODO: Separate debug/viz class, separate this func into bbox and 2d 3d points
-    def viz_object_states(self, cv_image, boxes, extracted_pts_all):
-        # Extract 3D pedestrians points in lidar frame
-        # ** These are camera frame after transform_lidar_points, right?
-        pedestrians_3d_pts = [list(extracted_pts[:, -3:]) for extracted_pts in extracted_pts_all] 
-
-        # Object center viz
-        obj_3d_obj_centers = list()
-        for track_id, agent in self.current_agents.items():
-            if agent.pose.x != None and agent.pose.y != None and agent.pose.z != None:
-                obj_3d_obj_centers.append((agent.pose.x, agent.pose.y, agent.pose.z)) # **
-        if len(obj_3d_obj_centers) > 0:
-            ros_obj_3d_obj_centers_pc2 = create_point_cloud(obj_3d_obj_centers, color=(0, 128, 0))
-            self.pub_obj_centers_pc2.publish(ros_obj_3d_obj_centers_pc2)
-        
-        # Extract 2D pedestrians points and bbox in camera frame
-        extracted_2d_pts = [list(extracted_pts[:, :2].astype(int)) for extracted_pts in extracted_pts_all]
-        flattened_pedestrians_2d_pts = list()
-        for pts in extracted_2d_pts:    flattened_pedestrians_2d_pts.extend(pts)
-
-        for ind, bbox in enumerate(boxes):
-            xywh = bbox.xywh[0].tolist()
-            cv_image = vis_2d_bbox(cv_image, xywh, bbox)
-        
-        flattened_pedestrians_3d_pts = list() 
-        for pts in pedestrians_3d_pts: flattened_pedestrians_3d_pts.extend(pts)
-
-        # Draw projected 2D LiDAR points on the image.
-        for pt in flattened_pedestrians_2d_pts:
-            cv2.circle(cv_image, pt, 2, (0, 0, 255), -1)
-        ros_img = self.bridge.cv2_to_imgmsg(cv_image, 'bgr8')
-        self.pub_image.publish(ros_img)  
-
-        # Draw 3D pedestrian pointclouds
-        # Create point cloud from extracted 3D points
-        ros_extracted_pedestrian_pc2 = create_point_cloud(flattened_pedestrians_3d_pts)
-        self.pub_pedestrians_pc2.publish(ros_extracted_pedestrian_pc2)
-        
-
-    def update_object_states(self, track_result, extracted_pts_all) -> None:
-        self.prev_agents = self.current_agents.copy()
-        self.current_agents.clear() 
-
-        # Change pedestrians_3d_pts to dicts matching track_ids
-        track_ids = track_result[0].boxes.id.int().cpu().tolist()
-        num_objs = len(track_ids)
-        boxes = track_result[0].boxes
-
-        # Extract 3D pedestrians points in lidar frame
-        # ** These are camera frame after transform_lidar_points, right?
-        pedestrians_3d_pts = [list(extracted_pts[:, -3:]) for extracted_pts in extracted_pts_all] 
-        if len(pedestrians_3d_pts) != num_objs:
-            raise Exception('Perception - Camera detections, points clusters num. mismatch')
-        
-        # TODO: Slower but cleaner to pass dicts of AgentState
-        #       or at least {track_ids: centers/pts/etc}
-        # TODO: Combine funcs for efficiency in C.
-        #       Separate numpy prob still faster for now
-        obj_centers = self.find_centers(pedestrians_3d_pts)
-        obj_dims = self.find_dims(pedestrians_3d_pts)
-        obj_vels = self.find_vels(track_ids, obj_centers)
-
-        # Update Current AgentStates
-        for ind in range(num_objs):
-            obj_center = (None, None, None) if obj_centers[ind].size == 0 else obj_centers[ind]
-            obj_dim = (None, None, None) if obj_dims[ind].size == 0 else obj_dims[ind]
-            self.current_agents[track_ids[ind]] = (
-                AgentState(
-                    track_id = track_ids[ind],
-                    pose=ObjectPose(t=0, x=obj_center[0], y=obj_center[1], z=obj_center[2] ,yaw=0,pitch=0,roll=0,frame=ObjectFrameEnum.CURRENT),
-                    # (l, w, h)
-                    # TODO: confirm (z -> l, x -> w, y -> h)
-                    dimensions=(obj_dim[2], obj_dim[0],obj_dim[1]),  
-                    outline=None,
-                    type=AgentEnum.PEDESTRIAN,
-                    activity=AgentActivityEnum.MOVING,
-                    velocity= None if obj_vels[track_ids[ind]].size == 0 else tuple(vels[track_ids[ind]]),
-                    yaw_rate=0
-                ))
-
-
-    def ouster_oak_callback(self, rgb_image_msg: Image, lidar_pc2_msg: PointCloud2):
-        # Convert to cv2 image and run detector
-        cv_image = self.bridge.imgmsg_to_cv2(rgb_image_msg, "bgr8") 
-        track_result = self.detector.track(source=cv_image, classes=self.classes_to_detect, persist=True, conf=self.confidence)
-
-        # Convert 1D PointCloud2 data to x, y, z coords
-        lidar_points = convert_pointcloud2_to_xyz(lidar_pc2_msg)
-        # print("len lidar_points", len(lidar_points))
-
-        # Downsample xyz point clouds
-        downsampled_points = downsample_points(lidar_points)
-        # print("len downsampled_points", len(downsampled_points))
-        
-        # Transform LiDAR points into the camera coordinate frame.
-        lidar_in_camera = transform_lidar_points(downsampled_points, self.R, self.t)
-        # print("len lidar_in_camera", len(lidar_in_camera))
-
-        # Project the transformed points into the image plane.
-        projected_pts = project_points(lidar_in_camera, self.K)
-        # print("len projected_pts", len(projected_pts))
-
-        # Process bboxes
-        boxes = track_result[0].boxes
-
-        extracted_pts_all = list()
-
-        for ind, bbox in enumerate(boxes):
-            xywh = bbox.xywh[0].tolist()
-
-            # Extracting projected pts
-            x, y, w, h = xywh
-            left_bound = int(x - w / 2)
-            right_bound = int(x + w / 2)
-            top_bound = int(y - h / 2)
-            bottom_bound = int(y + h / 2)
-
-            pts = np.array(projected_pts)
-            extracted_pts = pts[(pts[:, 0] > left_bound) &
-                                (pts[:, 0] < right_bound) &
-                                (pts[:, 1] > top_bound) &
-                                (pts[:, 1] < bottom_bound)
-                                ]
-
-            # Apply ground and max distance filter to the extracted 5D points
-            extracted_pts = filter_ground_points(extracted_pts, self.ground_threshold)
-            extracted_pts = filter_far_points(extracted_pts)
-            extracted_pts_all.append(extracted_pts)
-            
-        self.update_object_states(track_result, extracted_pts_all)
-        if self.debug: self.viz_object_states(cv_image, boxes, extracted_pts_all)
-
-
-if __name__ == '__main__':
-    rospy.init_node('fusion_node', anonymous=True)
-    Fusion3D()
-    while not rospy.core.is_shutdown():
-        rospy.rostime.wallsleep(0.5)
diff --git a/GEMstack/onboard/perception/pedestrian_detection.py b/GEMstack/onboard/perception/pedestrian_detection.py
index 1f8cfe5b..bd43494d 100644
--- a/GEMstack/onboard/perception/pedestrian_detection.py
+++ b/GEMstack/onboard/perception/pedestrian_detection.py
@@ -39,6 +39,7 @@ import message_filters
 import tf
 from cv_bridge import CvBridge
 from sensor_msgs.msg import Image, PointCloud2
+from visualization_msgs.msg import MarkerArray
 # YOLO
 from ultralytics import YOLO
 from ultralytics.engine.results import Results, Boxes
@@ -83,7 +84,7 @@ class PedestrianDetector2D(Component):
         self.confidence = 0.7
         self.classes_to_detect = 0
         self.ground_threshold = 1.6
-        self.max_dist_percent = 0.7
+        self.max_human_depth = 0.9
 
         # Load calibration data
         # TODO: Maybe lets add one word or link what R t K are?
@@ -106,6 +107,7 @@ class PedestrianDetector2D(Component):
          # Debug Publishers
         self.pub_pedestrians_pc2 = rospy.Publisher("/point_cloud/pedestrians", PointCloud2, queue_size=10)
         self.pub_obj_centers_pc2 = rospy.Publisher("/point_cloud/obj_centers", PointCloud2, queue_size=10)
+        self.pub_bboxes_markers = rospy.Publisher("/markers/bboxes", MarkerArray, queue_size=10)
         self.pub_image = rospy.Publisher("/camera/image_detection", Image, queue_size=1)
 
     def update(self, vehicle : VehicleState) -> Dict[str,AgentState]:
@@ -152,19 +154,19 @@ class PedestrianDetector2D(Component):
 
 
     # TODO: Separate debug/viz class, bbox and 2d 3d points funcs 
-    def viz_object_states(self, cv_image: cv2.typing.MatLike, boxes: Boxes, extracted_pts_all: List[np.ndarray]) -> None:
+    def viz_object_states(self, cv_image, boxes, extracted_pts_all):
         # Extract 3D pedestrians points in lidar frame
         # ** These are camera frame after transform_lidar_points, right?
         pedestrians_3d_pts = [list(extracted_pts[:, -3:]) for extracted_pts in extracted_pts_all] 
 
         # Object center viz
         obj_3d_obj_centers = list()
+        obj_3d_obj_dims = list()
         for track_id, agent in self.current_agents.items():
             if agent.pose.x != None and agent.pose.y != None and agent.pose.z != None:
                 obj_3d_obj_centers.append((agent.pose.x, agent.pose.y, agent.pose.z)) # **
-        if len(obj_3d_obj_centers) > 0:
-            ros_obj_3d_obj_centers_pc2 = create_point_cloud(obj_3d_obj_centers, color=(0, 128, 0))
-            self.pub_obj_centers_pc2.publish(ros_obj_3d_obj_centers_pc2)
+            if agent.dimensions != None and agent.dimensions[0] != None and agent.dimensions[1] != None and agent.dimensions[2] != None:
+                obj_3d_obj_dims.append(agent.dimensions)
         
         # Extract 2D pedestrians points and bbox in camera frame
         extracted_2d_pts = [list(extracted_pts[:, :2].astype(int)) for extracted_pts in extracted_pts_all]
@@ -178,21 +180,39 @@ class PedestrianDetector2D(Component):
         flattened_pedestrians_3d_pts = list() 
         for pts in pedestrians_3d_pts: flattened_pedestrians_3d_pts.extend(pts)
 
-        # Draw projected 2D LiDAR points on the image.
-        for pt in flattened_pedestrians_2d_pts:
-            cv2.circle(cv_image, pt, 2, (0, 0, 255), -1)
-        ros_img = self.bridge.cv2_to_imgmsg(cv_image, 'bgr8')
-        self.pub_image.publish(ros_img)  
+        if len(flattened_pedestrians_3d_pts) > 0:
+            # Draw projected 2D LiDAR points on the image.
+            for pt in flattened_pedestrians_2d_pts:
+                cv2.circle(cv_image, pt, 2, (0, 0, 255), -1)
+            ros_img = self.bridge.cv2_to_imgmsg(cv_image, 'bgr8')
+            self.pub_image.publish(ros_img)  
+
+            # Draw 3D pedestrian pointclouds
+            # Create point cloud from extracted 3D points
+            ros_extracted_pedestrian_pc2 = create_point_cloud(flattened_pedestrians_3d_pts)
+            self.pub_pedestrians_pc2.publish(ros_extracted_pedestrian_pc2)
+
+        # Draw 3D pedestrian centers and dimensions
+        if len(obj_3d_obj_centers) > 0 and len(obj_3d_obj_dims) > 0:
+            # Draw 3D pedestrian center pointclouds
+            ros_obj_3d_obj_centers_pc2 = create_point_cloud(obj_3d_obj_centers, color=(0, 128, 0))
+            self.pub_obj_centers_pc2.publish(ros_obj_3d_obj_centers_pc2)
 
-        # Draw 3D pedestrian pointclouds
-        # Create point cloud from extracted 3D points
-        ros_extracted_pedestrian_pc2 = create_point_cloud(flattened_pedestrians_3d_pts)
-        self.pub_pedestrians_pc2.publish(ros_extracted_pedestrian_pc2)
+            # Draw 3D pedestrian bboxes markers
+            # Create bbox marker from pedestrain dimensions
+            ros_delete_bboxes_markers = delete_bbox_marker()
+            self.pub_bboxes_markers.publish(ros_delete_bboxes_markers)
+            ros_pedestrians_bboxes_markers = create_bbox_marker(obj_3d_obj_centers, obj_3d_obj_dims)
+            self.pub_bboxes_markers.publish(ros_pedestrians_bboxes_markers)
         
 
     def update_object_states(self, track_result: List[Results], extracted_pts_all: List[np.ndarray]) -> None:
         self.prev_agents = self.current_agents.copy()
-        self.current_agents.clear() 
+        self.current_agents.clear()
+
+        # Return if no track results available
+        if track_result[0].boxes.id == None:
+            return
 
         # Change pedestrians_3d_pts to dicts matching track_ids
         track_ids = track_result[0].boxes.id.int().cpu().tolist()
@@ -223,15 +243,15 @@ class PedestrianDetector2D(Component):
                     pose=ObjectPose(t=0, x=obj_center[0], y=obj_center[1], z=obj_center[2] ,yaw=0,pitch=0,roll=0,frame=ObjectFrameEnum.CURRENT),
                     # (l, w, h)
                     # TODO: confirm (z -> l, x -> w, y -> h)
-                    dimensions=(obj_dim[2], obj_dim[0],obj_dim[1]),  
+                    dimensions=(obj_dim[0], obj_dim[1], obj_dim[2]),  
                     outline=None,
                     type=AgentEnum.PEDESTRIAN,
                     activity=AgentActivityEnum.MOVING,
-                    velocity= None if obj_vels[track_ids[ind]].size == 0 else tuple(vels[track_ids[ind]]),
+                    velocity= None if obj_vels[track_ids[ind]].size == 0 else tuple(obj_vels[track_ids[ind]]),
                     yaw_rate=0
                 ))
 
-    def ouster_oak_callback(self, rgb_image_msg: Image, lidar_pc2_msg: PointCloud2) -> None:
+    def ouster_oak_callback(self, rgb_image_msg: Image, lidar_pc2_msg: PointCloud2):
         # Convert to cv2 image and run detector
         cv_image = self.bridge.imgmsg_to_cv2(rgb_image_msg, "bgr8") 
         track_result = self.detector.track(source=cv_image, classes=self.classes_to_detect, persist=True, conf=self.confidence)
@@ -254,7 +274,9 @@ class PedestrianDetector2D(Component):
 
         # Process bboxes
         boxes = track_result[0].boxes
+
         extracted_pts_all = list()
+
         for ind, bbox in enumerate(boxes):
             xywh = bbox.xywh[0].tolist()
 
@@ -266,19 +288,23 @@ class PedestrianDetector2D(Component):
             bottom_bound = int(y + h / 2)
 
             pts = np.array(projected_pts)
-            extracted_pts = pts[(pts[:, 0] > left_bound) &
-                                (pts[:, 0] < right_bound) &
-                                (pts[:, 1] > top_bound) &
-                                (pts[:, 1] < bottom_bound)
-                                ]
-
-            # Apply ground and max distance filter to the extracted 5D points
-            extracted_pts = filter_ground_points(extracted_pts, self.ground_threshold)
-            extracted_pts = filter_far_points(extracted_pts)
-            extracted_pts_all.append(extracted_pts)
-
-        self.update_object_states(track_result, extracted_pts_all)
-        if self.debug: self.viz_object_states(cv_image, boxes, extracted_pts_all)
+            # Checking projected_pts length is very important
+            if len(projected_pts) > 0:
+                extracted_pts = pts[(pts[:, 0] > left_bound) &
+                                    (pts[:, 0] < right_bound) &
+                                    (pts[:, 1] > top_bound) &
+                                    (pts[:, 1] < bottom_bound)
+                                    ]
+
+                # Apply ground and max distance filter to the extracted 5D points
+                extracted_pts = filter_ground_points(extracted_pts, self.ground_threshold)
+                extracted_pts = filter_depth_points(extracted_pts)
+                extracted_pts_all.append(extracted_pts)
+        
+        if len(extracted_pts_all) > 0 and len(track_result) > 0:
+            self.update_object_states(track_result, extracted_pts_all)
+            if self.debug: self.viz_object_states(cv_image, boxes, extracted_pts_all)
+
 
     def rate(self):
         return 4.0
diff --git a/GEMstack/onboard/perception/pedestrian_detection_utils.py b/GEMstack/onboard/perception/pedestrian_detection_utils.py
index 4163d7b2..c7460323 100644
--- a/GEMstack/onboard/perception/pedestrian_detection_utils.py
+++ b/GEMstack/onboard/perception/pedestrian_detection_utils.py
@@ -1,4 +1,5 @@
 from sensor_msgs.msg import PointCloud2, PointField
+from visualization_msgs.msg import Marker, MarkerArray
 import numpy as np
 import sensor_msgs.point_cloud2 as pc2
 import open3d as o3d
@@ -35,11 +36,15 @@ def filter_ground_points(lidar_points, ground_threshold = 0):
     return filtered_array
 
 
-def filter_far_points(lidar_points, max_dist_percent=0.85):
-    """ Filter points beyond a percentage threshold of max distance in a point cluster """
+def filter_depth_points(lidar_points, max_human_depth=0.9):
+    """ Filter points beyond a max possible human depth in a point cluster """
     if lidar_points.shape[0] == 0: return lidar_points
-    max_dist = np.max(lidar_points[:, 4])
-    filtered_array = lidar_points[lidar_points[:, 4] < max_dist_percent * max_dist]
+    lidar_points_dist = lidar_points[:, 4]
+    min_dist = np.min(lidar_points_dist)
+    max_dist = np.max(lidar_points_dist)
+    max_possible_dist = min_dist + max_human_depth
+    actual_dist = min(max_dist, max_possible_dist)
+    filtered_array = lidar_points[lidar_points_dist < actual_dist]
     return filtered_array
 
 
@@ -167,4 +172,73 @@ def create_point_cloud(points, color=(255, 0, 0)):
 
     return pc2.create_cloud(header, fields, point_cloud_data)
 
+
+def create_bbox_marker(centroids, dimensions):
+    """
+    Create 3D bbox markers from centroids and dimensions
+    """
+    marker_array = MarkerArray()
+
+    for i, (centroid, dimension) in enumerate(zip(centroids, dimensions)):
+        # Skip if no centroid or dimension
+        if (centroid == None) or (dimension == None):
+            continue
+
+        marker = Marker()
+        marker.header.frame_id = "map"  # Reference frame
+        marker.header.stamp = rospy.Time.now()
+        marker.ns = "bounding_boxes"
+        marker.id = i  # Unique ID for each marker
+        marker.type = Marker.CUBE  # Cube for bounding box
+        marker.action = Marker.ADD
+
+        # Position (center of the bounding box)
+        c_x, c_y, c_z = centroid
+        if (not isinstance(c_x, float)) or (not isinstance(c_y, float)) or (not isinstance(c_z, float)):
+            continue
+
+        marker.pose.position.x = c_x
+        marker.pose.position.y = c_y
+        marker.pose.position.z = c_z
+
+        # Orientation (default, no rotation)
+        marker.pose.orientation.x = 0.0
+        marker.pose.orientation.y = 0.0
+        marker.pose.orientation.z = 0.0
+        marker.pose.orientation.w = 1.0
+
+        # Bounding box dimensions
+        d_x, d_y, d_z = dimension
+        if (not isinstance(d_x, float)) or (not isinstance(d_y, float)) or (not isinstance(d_z, float)):
+            continue
+
+        marker.scale.x = d_x
+        marker.scale.y = d_y
+        marker.scale.z = d_z
+
+        # Random colors for each bounding box
+        marker.color.r = 0.0  # Varying colors
+        marker.color.g = 1.0
+        marker.color.b = 1.5
+        marker.color.a = 0.2  # Transparency
+
+        marker.lifetime = rospy.Duration()  # Persistent
+        marker_array.markers.append(marker)
+    return marker_array
+
+
+def delete_bbox_marker():
+    """
+    Delete 3D bbox markers given ID ranges
+    """
+    marker_array = MarkerArray()
+    for i in range(6):
+        marker = Marker()
+        marker.ns = "bounding_boxes"
+        marker.id = i
+        marker.action = Marker.DELETE
+        marker_array.markers.append(marker)
+    return marker_array
+
+
     
\ No newline at end of file
-- 
2.38.1

