From 68d0a75c714f7db8eea85691d79c113214903a3b Mon Sep 17 00:00:00 2001
From: KenC1014 <kenken4016@gmail.com>
Date: Mon, 17 Feb 2025 22:03:36 -0600
Subject: [PATCH 042/150] fix lidar frame mapping

---
 .../perception/pedestrian_detection.py        | 13 ++++++------
 .../perception/pedestrian_detection_utils.py  | 20 +++++++++----------
 GEMstack/onboard/perception/transform.py      |  6 ++++--
 3 files changed, 21 insertions(+), 18 deletions(-)

diff --git a/GEMstack/onboard/perception/pedestrian_detection.py b/GEMstack/onboard/perception/pedestrian_detection.py
index 947bd125..6170b578 100644
--- a/GEMstack/onboard/perception/pedestrian_detection.py
+++ b/GEMstack/onboard/perception/pedestrian_detection.py
@@ -83,7 +83,7 @@ class PedestrianDetector2D(Component):
         self.curr_time = 1 # Avoid divide by 0 for placebolder, 0
         self.confidence = 0.7
         self.classes_to_detect = 0
-        self.ground_threshold = 1.6
+        self.ground_threshold = -2.0
         self.max_human_depth = 0.9
 
         # Load calibration data
@@ -162,7 +162,7 @@ class PedestrianDetector2D(Component):
     def viz_object_states(self, cv_image, boxes, extracted_pts_all):
         # Extract 3D pedestrians points in lidar frame
         # ** These are camera frame after transform_lidar_points, right?
-        pedestrians_3d_pts = [list(extracted_pts[:, -3:]) for extracted_pts in extracted_pts_all] 
+        pedestrians_3d_pts = [list(extracted_pts[:, -3:]) for extracted_pts in extracted_pts_all]
 
         # Object center viz
         obj_3d_obj_centers = list()
@@ -196,7 +196,8 @@ class PedestrianDetector2D(Component):
             # Tranform 3D pedestrians points to vehicle frame for better visualization. Turn off for performance
             flattened_pedestrians_3d_pts_vehicle = transform_lidar_points(np.array(flattened_pedestrians_3d_pts), self.R_lidar_to_vehicle, self.t_lidar_to_vehicle)
             # Create point cloud from extracted 3D points
-            ros_extracted_pedestrian_pc2 = create_point_cloud(flattened_pedestrians_3d_pts_vehicle)
+            # ros_extracted_pedestrian_pc2 = create_point_cloud(flattened_pedestrians_3d_pts_vehicle)
+            ros_extracted_pedestrian_pc2 = create_point_cloud(flattened_pedestrians_3d_pts)
             self.pub_pedestrians_pc2.publish(ros_extracted_pedestrian_pc2)
 
         # Draw 3D pedestrian centers and dimensions
@@ -287,8 +288,8 @@ class PedestrianDetector2D(Component):
         # print("len lidar_in_camera", len(lidar_in_camera))
 
         # Project the transformed points into the image plane.
-        projected_pts = project_points(lidar_in_camera, self.K)
-        # print("len projected_pts", len(projected_pts))
+        projected_pts = project_points(lidar_in_camera, self.K, downsampled_points)
+        # print("projected_pts", len(projected_pts))
 
         # Process bboxes
         boxes = track_result[0].boxes
@@ -316,7 +317,7 @@ class PedestrianDetector2D(Component):
 
                 # Apply ground and max distance filter to the extracted 5D points
                 extracted_pts = filter_ground_points(extracted_pts, self.ground_threshold)
-                extracted_pts = filter_depth_points(extracted_pts)
+                extracted_pts = filter_depth_points(extracted_pts, self.max_human_depth)
                 extracted_pts_all.append(extracted_pts)
         
         if len(extracted_pts_all) > 0 and len(track_result) > 0:
diff --git a/GEMstack/onboard/perception/pedestrian_detection_utils.py b/GEMstack/onboard/perception/pedestrian_detection_utils.py
index c7460323..788ee2e5 100644
--- a/GEMstack/onboard/perception/pedestrian_detection_utils.py
+++ b/GEMstack/onboard/perception/pedestrian_detection_utils.py
@@ -32,14 +32,14 @@ def downsample_points(lidar_points):
 
 def filter_ground_points(lidar_points, ground_threshold = 0):
     """ Filter points given an elevation of ground threshold """
-    filtered_array = lidar_points[lidar_points[:, 3] < ground_threshold]
+    filtered_array = lidar_points[lidar_points[:, 4] > ground_threshold]
     return filtered_array
 
 
 def filter_depth_points(lidar_points, max_human_depth=0.9):
     """ Filter points beyond a max possible human depth in a point cluster """
     if lidar_points.shape[0] == 0: return lidar_points
-    lidar_points_dist = lidar_points[:, 4]
+    lidar_points_dist = lidar_points[:, 2]
     min_dist = np.min(lidar_points_dist)
     max_dist = np.max(lidar_points_dist)
     max_possible_dist = min_dist + max_human_depth
@@ -97,19 +97,19 @@ def transform_lidar_points(lidar_points, R, t):
     return P_cam
 
 
-def project_points(points_3d, K):
+def project_points(points_3d, K, lidar_points):
     """
     Project 3D points (in the camera frame) into 2D image coordinates using the camera matrix K.
     Only projects points with z > 0.
     """
     proj_points = []
-    for pt in points_3d:
+    for pt, l_pt in zip(points_3d, lidar_points):
         if pt[2] > 0:  # only project points in front of the camera
             u = K[0, 0] * (pt[0] / pt[2]) + K[0, 2]
             v = K[1, 1] * (pt[1] / pt[2]) + K[1, 2]
-            # 5D points that stores original lidar points
-            proj_points.append((int(u), int(v), pt[0], pt[1], pt[2]))
-    return proj_points
+            # 5D data point
+            proj_points.append((int(u), int(v), l_pt[0], l_pt[1], l_pt[2]))
+    return np.array(proj_points)
 
 
 def vis_2d_bbox(image, xywh, box):
@@ -155,7 +155,7 @@ def create_point_cloud(points, color=(255, 0, 0)):
     """
     header = rospy.Header()
     header.stamp = rospy.Time.now()
-    header.frame_id = "map"  # Change to your TF frame
+    header.frame_id = "os_sensor"  # Change to your TF frame
 
     fields = [
         PointField(name="x", offset=0, datatype=PointField.FLOAT32, count=1),
@@ -185,7 +185,7 @@ def create_bbox_marker(centroids, dimensions):
             continue
 
         marker = Marker()
-        marker.header.frame_id = "map"  # Reference frame
+        marker.header.frame_id = "os_sensor"  # Reference frame
         marker.header.stamp = rospy.Time.now()
         marker.ns = "bounding_boxes"
         marker.id = i  # Unique ID for each marker
@@ -241,4 +241,4 @@ def delete_bbox_marker():
     return marker_array
 
 
-    
\ No newline at end of file
+    
diff --git a/GEMstack/onboard/perception/transform.py b/GEMstack/onboard/perception/transform.py
index 6e921f56..b876cc59 100644
--- a/GEMstack/onboard/perception/transform.py
+++ b/GEMstack/onboard/perception/transform.py
@@ -9,8 +9,10 @@ def publish_tf():
 
     while not rospy.is_shutdown():
         br.sendTransform(
-            (0, 1.6, 7),  # (x, y, z) translation
-            tf.transformations.quaternion_from_euler(0.5* np.pi, 0, 0),  # (roll, pitch, yaw)
+            # (0, 1.6, 7),  # (x, y, z) translation
+            # tf.transformations.quaternion_from_euler(0.5* np.pi, 0, 0),  # (roll, pitch, yaw)
+            (0, 0, 0),  # (x, y, z) translation
+            tf.transformations.quaternion_from_euler(0, 0, 0),  # (roll, pitch, yaw)
             rospy.Time.now(),
             "os_sensor",  # Child frame (sensor)
             "map"  # Parent frame (world)
-- 
2.38.1

