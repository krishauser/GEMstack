From c31546dba56640a2794585b7efcc49af9c315e5f Mon Sep 17 00:00:00 2001
From: KenC1014 <kenken4016@gmail.com>
Date: Sat, 15 Feb 2025 16:35:57 -0600
Subject: [PATCH 032/150] add comments

---
 GEMstack/onboard/perception/fusion.py       | 6 +++---
 GEMstack/onboard/perception/fusion_utils.py | 4 +++-
 2 files changed, 6 insertions(+), 4 deletions(-)

diff --git a/GEMstack/onboard/perception/fusion.py b/GEMstack/onboard/perception/fusion.py
index 18f39093..bfd39f7c 100644
--- a/GEMstack/onboard/perception/fusion.py
+++ b/GEMstack/onboard/perception/fusion.py
@@ -90,12 +90,15 @@ class Fusion3D():
                 if len(extracted_pts) < 1:
                     continue
                 
+                # Apply ground and max distance filter to extracted 5D points
                 extracted_pts = filter_ground_points(extracted_pts, self.ground_threshold)
                 extracted_pts = filter_far_points(extracted_pts)
                 
+                # Extract 2D pedestrians points camera frame
                 extracted_2d_pts = list(extracted_pts[:, :2].astype(int))
                 flattened_pedestrians_2d_pts = flattened_pedestrians_2d_pts + extracted_2d_pts
 
+                # Extract 3D pedestrians points in lidar frame
                 extracted_3d_pts = list(extracted_pts[:, -3:])
                 pedestrians_3d_pts.append(extracted_3d_pts)
                 flattened_pedestrians_3d_pts = flattened_pedestrians_3d_pts + extracted_3d_pts
@@ -105,9 +108,6 @@ class Fusion3D():
                 cv_image = vis_2d_bbox(cv_image, xywh, box)
         
         if len(flattened_pedestrians_3d_pts) > 0:
-            # print(f"x_dim pedestrians_3d_pts: {np.max(np.array(flattened_pedestrians_3d_pts)[:, 0]), np.min(np.array(flattened_pedestrians_3d_pts)[:, 0])}")
-            # print(f"y_dim pedestrians_3d_pts: {np.max(np.array(flattened_pedestrians_3d_pts)[:, 1]), np.min(np.array(flattened_pedestrians_3d_pts)[:, 1])}")
-            # print(f"z_dim pedestrians_3d_pts: {np.max(np.array(flattened_pedestrians_3d_pts)[:, 2]), np.min(np.array(flattened_pedestrians_3d_pts)[:, 2])}")
             # Draw projected 2D LiDAR points on the image.
             for pt in flattened_pedestrians_2d_pts:
                 cv2.circle(cv_image, pt, 2, (0, 0, 255), -1)
diff --git a/GEMstack/onboard/perception/fusion_utils.py b/GEMstack/onboard/perception/fusion_utils.py
index 4aacb481..cd6fd1ce 100644
--- a/GEMstack/onboard/perception/fusion_utils.py
+++ b/GEMstack/onboard/perception/fusion_utils.py
@@ -1,5 +1,4 @@
 from sensor_msgs.msg import PointCloud2, PointField
-from scipy.stats import zscore
 import numpy as np
 import sensor_msgs.point_cloud2 as pc2
 import open3d as o3d
@@ -31,11 +30,13 @@ def downsample_points(lidar_points):
 
 
 def filter_ground_points(lidar_points, ground_threshold = 0):
+    """ Filter points given an elevation of ground threshold """
     filtered_array = lidar_points[lidar_points[:, 3] < ground_threshold]
     return filtered_array
 
 
 def filter_far_points(lidar_points, max_dist_percent=0.85):
+    """ Filter points beyond a percentage threshold of max distance in a point cluster """
     max_dist = np.max(lidar_points[:, 4])
     filtered_array = lidar_points[lidar_points[:, 4] < max_dist_percent * max_dist]
     return filtered_array
@@ -100,6 +101,7 @@ def project_points(points_3d, K):
         if pt[2] > 0:  # only project points in front of the camera
             u = K[0, 0] * (pt[0] / pt[2]) + K[0, 2]
             v = K[1, 1] * (pt[1] / pt[2]) + K[1, 2]
+            # 5D points that stores original lidar points
             proj_points.append((int(u), int(v), pt[0], pt[1], pt[2]))
     return proj_points
 
-- 
2.38.1

